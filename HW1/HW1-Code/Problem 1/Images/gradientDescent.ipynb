{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Fri Apr  2 22:15:13 2021\n",
    "\n",
    "@author: Alireza Khayyatian\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "''' Define gradient descent function  ''' \n",
    "\n",
    "def gradientDescent(x, y,x_test,y_test, theta_MSE, theta_MAE, theta_RMSE, alpha, m,s, numIterations\n",
    "                    ,error_train_MSE,error_train_MAE,error_train_RMSE\n",
    "                    ,error_test_MSE , error_test_MAE , error_test_RMSE\n",
    "                    ,gradient_MSE_set,gradient_MAE_set,gradient_RMSE_set):\n",
    "    xTrans = x.transpose()\n",
    "    for i in range(0, numIterations):\n",
    "        \n",
    "        hypothesis_MSE = np.dot(x, theta_MSE)\n",
    "        hypothesis_MAE = np.dot(x, theta_MAE)\n",
    "        hypothesis_RMSE = np.dot(x, theta_RMSE)\n",
    "        \n",
    "        loss_MSE = hypothesis_MSE - y\n",
    "        loss_MAE = hypothesis_MAE - y\n",
    "        loss_RMSE = hypothesis_RMSE - y\n",
    "\n",
    "        # avg cost per example (the 2 in 2*m doesn't really matter here.\n",
    "        # But to be consistent with the gradient, I include it)\n",
    "        \n",
    "        cost_MSE = np.sum(loss_MSE ** 2) / (2 * m) #MSE\n",
    "        cost_MAE = np.sum(np.abs(loss_MAE)) / m  #MAE\n",
    "        cost_RMSE = np.sqrt(np.sum(loss_RMSE ** 2) /  m) #RMSE\n",
    "        \n",
    "        #print(\"Iteration %d | Cost: %f\" % (i, cost))\n",
    "        # avg gradient per example\n",
    "        gradient_MSE = np.dot(xTrans, loss_MSE) / m\n",
    "        gradient_MAE = np.dot( xTrans, np.sign(loss_MAE) ) / m\n",
    "        gradient_RMSE = np.dot(xTrans, loss_RMSE) / (m*cost_RMSE)\n",
    "        # update\n",
    "        theta_MSE = theta_MSE - alpha * gradient_MSE\n",
    "        theta_MAE = theta_MAE - alpha * gradient_MAE\n",
    "        theta_RMSE = theta_RMSE - alpha * gradient_RMSE\n",
    "\n",
    "        \n",
    "        error_train_MSE.append(cost_MSE)\n",
    "        error_train_MAE.append(cost_MAE)\n",
    "        error_train_RMSE.append(cost_RMSE)\n",
    "\n",
    "        \n",
    "        gradient_MSE_set.append(gradient_MSE)\n",
    "        gradient_MAE_set.append(gradient_MAE)\n",
    "        gradient_RMSE_set.append(gradient_RMSE)\n",
    "        \n",
    "        #calculate Test Error    \n",
    "        hypothesis_test_MSE = np.dot(x_test, theta_MSE)\n",
    "        hypothesis_test_MAE = np.dot(x_test, theta_MAE)\n",
    "        hypothesis_test_RMSE = np.dot(x_test, theta_RMSE)\n",
    "\n",
    "\n",
    "        loss_test_MSE = hypothesis_test_MSE - y_test\n",
    "        loss_test_MAE = hypothesis_test_MAE - y_test\n",
    "        loss_test_RMSE = hypothesis_test_RMSE - y_test\n",
    "\n",
    "        # avg cost per example (the 2 in 2*m doesn't really matter here.\n",
    "        # But to be consistent with the gradient, I include it)\n",
    "        \n",
    "        cost_test_MSE = np.sum(loss_test_MSE ** 2) / (2 * s) #MSE\n",
    "        cost_test_MAE = np.sum(np.abs(loss_test_MAE)) / s  #MAE\n",
    "        cost_test_RMSE = np.sqrt(np.sum(loss_test_RMSE ** 2) /  s) #RMSE\n",
    "\n",
    "\n",
    "        error_test_MSE.append(cost_test_MSE)\n",
    "        error_test_MAE.append(cost_test_MAE)\n",
    "        error_test_RMSE.append(cost_test_RMSE)\n",
    "\n",
    "    return (theta_MSE,theta_MAE,theta_RMSE)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
